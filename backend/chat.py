import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time

st.set_page_config(page_title="ChatLaw")
st.title("ChatLaw")

# é¢„å®šä¹‰çš„æ¨¡å‹åˆ—è¡¨ï¼ˆåç§°: è·¯å¾„ï¼‰
MODEL_OPTIONS = {
    "ChatLaw-æ³•å¾‹å’¨è¯¢": "D:\\models\\ChatLaw-13B",
    "LaWGPT-é€šç”¨æ³•å¾‹æ¨¡å‹": "D:\\models\\LaWGPT-7B",
    "ChatLaw-å¸æ³•æ‘˜è¦": "D:\\models\\Qwen2.5-0.5B-sfzy"
}


@st.cache_resource
def init_model(model_path):
    """æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è·¯å¾„åŠ è½½æ¨¡å‹"""
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,
            device_map="auto",
            torch_dtype="auto"
        )
        if torch.cuda.is_available():
            model.to('cuda')

        tokenizer = AutoTokenizer.from_pretrained(model_path)
        return model, tokenizer
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None, None


def clear_chat_history():
    del st.session_state.messages


def init_chat_history():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    return st.session_state.messages


def response_generator(response):
    for word in response:
        yield word
        time.sleep(0.02)


def main():
    # ä¾§è¾¹æ æ¨¡å‹é€‰æ‹©
    chosen_model = st.sidebar.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        options=list(MODEL_OPTIONS.keys()),
        index=0  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª
    )

    # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹åç§°è·å–è·¯å¾„
    model_path = MODEL_OPTIONS[chosen_model]
    model, tokenizer = init_model(model_path)

    if model is None or tokenizer is None:
        st.stop()

    # åˆå§‹åŒ–èŠå¤©è®°å½•
    messages = init_chat_history()

    # æ˜¾ç¤ºæ¬¢è¿è¯­ï¼ˆä»…åœ¨é¦–æ¬¡åŠ è½½æ—¶ï¼‰
    if not messages:
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            st.markdown(f"æ‚¨å¥½ï¼Œæˆ‘æ˜¯{chosen_model}ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ’–")

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in messages:
        avatar = "ğŸ™‹â€â™‚ï¸" if message["role"] == "user" else "ğŸ¤–"
        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("Shift + Enter æ¢è¡Œï¼ŒEnter å‘é€"):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user", avatar="ğŸ™‹â€â™‚ï¸"):
            st.markdown(prompt)
        messages.append({"role": "user", "content": prompt})

        # ç”Ÿæˆå›å¤
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            try:
                # æ„å»ºæ¨¡å‹è¾“å…¥
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                )
                model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

                # ç”Ÿæˆæ–‡æœ¬
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=512,
                    pad_token_id=tokenizer.eos_token_id,
                    temperature=0.95,
                    do_sample=True,
                    top_p=0.7
                )

                # è§£ç å¹¶æå–æ–°ç”Ÿæˆå†…å®¹
                generated_ids = generated_ids[:, model_inputs.input_ids.shape[1]:]
                response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

                # æµå¼æ˜¾ç¤º
                st.write_stream(response_generator(response))
                messages.append({"role": "assistant", "content": response})

            except RuntimeError as e:
                if "CUDA out of memory" in str(e):
                    st.error("æ˜¾å­˜ä¸è¶³ï¼Œè¯·ç¼©çŸ­è¾“å…¥æˆ–é‡å¯ä¼šè¯ã€‚")
                else:
                    st.error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")

        # æ¸…ç©ºå¯¹è¯æŒ‰é’®
        st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    main()