import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time
from PyPDF2 import PdfReader
from docx import Document
import os
import random

st.set_page_config(page_title="LawLLM-å¸æ³•æ‘˜è¦", layout="wide")
st.title("ğŸ“‘ æ³•å¾‹æ–‡ä¹¦æ™ºèƒ½æ‘˜è¦ç³»ç»Ÿ")

# é¢„å®šä¹‰é…ç½®
MODEL_PATH = "D:\\models\\Qwen2.5-0.5B-sfzy"
SUPPORTED_FORMATS = ["pdf", "docx", "txt"]
MAX_FILE_SIZE = 10  # MB


@st.cache_resource
def init_model():
    """åˆå§‹åŒ–æ¨¡å‹"""
    try:
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            device_map="auto",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        return model, tokenizer
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None, None


def parse_uploaded_file(uploaded_file):
    """è§£æä¸Šä¼ æ–‡ä»¶"""
    file_type = uploaded_file.name.split('.')[-1].lower()

    if file_type == "pdf":
        return parse_pdf(uploaded_file)
    elif file_type == "docx":
        return parse_docx(uploaded_file)
    elif file_type == "txt":
        return uploaded_file.getvalue().decode("utf-8")
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼")


def parse_pdf(uploaded_file):
    """è§£æPDFæ–‡ä»¶"""
    try:
        pdf_reader = PdfReader(uploaded_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDFè§£æå¤±è´¥: {str(e)}")
        return ""


def parse_docx(uploaded_file):
    """è§£æWordæ–‡æ¡£"""
    try:
        doc = Document(uploaded_file)
        return "\n".join([para.text for para in doc.paragraphs])
    except Exception as e:
        st.error(f"DOCXè§£æå¤±è´¥: {str(e)}")
        return ""


def generate_summary(model, tokenizer, text):
    """ç”Ÿæˆæ–‡ä¹¦æ‘˜è¦"""
    try:
        # æ„å»ºæç¤ºè¯ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªæç¤ºè¯è¿›è¡Œæ‹¼æ¥
        prompt = random.choice(
            [
                f"""è¯·å½’çº³è¿™ç¯‡æ–‡ä¹¦çš„å¤§è‡´è¦ç‚¹ã€‚\n{text}""",
                f"""ä»¥ä¸‹æ˜¯ä¸€ç¯‡æ³•å¾‹æ–‡ä¹¦ï¼š\n{text}\nè¯·å¤§è‡´æè¿°è¿™ç¯‡æ–‡ä¹¦çš„å†…å®¹ã€‚""",
                f"""è¯·å¯¹è¿™ç¯‡æ³•å¾‹æ–‡ä¹¦è¿›è¡Œæ‘˜è¦\n\n{text}""",
                f"""{text}\nä»¥ä¸Šæ˜¯ä¸€ç¯‡æ³•å¾‹æ–‡ä¹¦ï¼Œè¯·å½’çº³è¿™ç¯‡æ–‡ä¹¦çš„å¤§è‡´è¦ç‚¹ã€‚"""
            ]
        )

        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            max_length=4096,
            truncation=True
        ).to(model.device)

        # ç”Ÿæˆå‚æ•°
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=512,
            temperature=0.95,
            do_sample=True,
            top_p=0.7,
            repetition_penalty=1.2,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id
        )

        # æå–ç”Ÿæˆå†…å®¹
        summary = tokenizer.decode(
            outputs[0][len(inputs.input_ids[0]):],
            skip_special_tokens=True
        )
        return summary
    except Exception as e:
        st.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
        return ""


def main():
    # åˆå§‹åŒ–æ¨¡å‹
    model, tokenizer = init_model()
    if model is None or tokenizer is None:
        st.stop()

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ³•å¾‹æ–‡ä¹¦",
            type=SUPPORTED_FORMATS,
            accept_multiple_files=False,
            help=f"æ”¯æŒæ ¼å¼ï¼š{', '.join(SUPPORTED_FORMATS)} | æœ€å¤§æ–‡ä»¶ï¼š{MAX_FILE_SIZE}MB",
        )

        # æ–‡ä»¶å¤§å°æ ¡éªŒ
        if uploaded_file and (uploaded_file.size > MAX_FILE_SIZE * 1024 * 1024):
            st.error(f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆæœ€å¤§ {MAX_FILE_SIZE}MBï¼‰")
            st.stop()

    # ä¸»ç•Œé¢
    col1, col2 = st.columns([1, 1], gap="large")

    with col1:
        st.subheader("ğŸ“¥ åŸæ–‡å†…å®¹")
        if uploaded_file:
            with st.spinner("æ­£åœ¨è§£ææ–‡æ¡£..."):
                raw_text = parse_uploaded_file(uploaded_file)
                if raw_text:
                    st.info(f"æ–‡æ¡£è§£ææˆåŠŸï¼ˆé•¿åº¦ï¼š{len(raw_text)}å­—ç¬¦ï¼‰")
                    with st.expander("æŸ¥çœ‹åŸæ–‡å†…å®¹", expanded=False):
                        st.text(raw_text[:2000] + ("..." if len(raw_text) > 2000 else ""))
                else:
                    st.warning("æ–‡æ¡£å†…å®¹ä¸ºç©º")

    with col2:
        st.subheader("ğŸ“ æ™ºèƒ½æ‘˜è¦")
        if uploaded_file and raw_text:
            start_analysis = st.button("å¼€å§‹ç”Ÿæˆæ‘˜è¦", type="primary")

            if start_analysis:
                with st.spinner("æ­£åœ¨ç”Ÿæˆä¸“ä¸šæ‘˜è¦..."):
                    start_time = time.time()
                    summary = generate_summary(model, tokenizer, raw_text)
                    elapsed = time.time() - start_time

                    st.success(f"æ‘˜è¦ç”Ÿæˆå®Œæˆï¼ˆè€—æ—¶ï¼š{elapsed:.1f}sï¼‰")
                    st.markdown("### æ‘˜è¦ç»“æœ")
                    st.write_stream(summary_generator(summary))

                    # æ·»åŠ ä¸‹è½½æŒ‰é’®
                    st.download_button(
                        label="ä¸‹è½½æ‘˜è¦",
                        data=summary,
                        file_name=f"{os.path.splitext(uploaded_file.name)[0]}_æ‘˜è¦.txt",
                        mime="text/plain"
                    )


def summary_generator(text):
    """æ‘˜è¦æµå¼ç”Ÿæˆæ•ˆæœ"""
    for word in text:
        yield word
        time.sleep(0.02)


if __name__ == "__main__":
    main()