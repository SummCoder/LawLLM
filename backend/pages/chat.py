import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time
import requests

st.set_page_config(page_title="ChatLaw")
st.title("ChatLaw")

# é¢„å®šä¹‰çš„æ¨¡å‹åˆ—è¡¨ï¼ˆåç§°: è·¯å¾„ï¼‰
MODEL_OPTIONS = {
    "ChatLaw-æ³•å¾‹å’¨è¯¢": "",
    "ChatLaw-å¸æ³•æ‘˜è¦": "D:\\models\\Qwen2.5-0.5B-sfzy"
}


@st.cache_resource
def init_model(model_path):
    """æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è·¯å¾„åŠ è½½æ¨¡å‹"""
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,
            device_map="auto",
            torch_dtype="auto"
        )
        if torch.cuda.is_available():
            model.to('cuda')

        tokenizer = AutoTokenizer.from_pretrained(model_path)
        return model, tokenizer
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None, None


def clear_chat_history():
    del st.session_state.messages


def init_chat_history():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    return st.session_state.messages


def response_generator(response):
    for word in response:
        yield word
        time.sleep(0.02)


# è·å–Access Tokençš„å‡½æ•°
def get_access_token():
    url = "https://chatglm.cn/chatglm/assistant-api/v1/get_token"
    data = {
        "api_key": "xxx",
        "api_secret": "xxx"
    }

    response = requests.post(url, json=data)
    token_info = response.json()
    return token_info['result']['access_token']


# è°ƒç”¨Assistantä¼šè¯çš„å‡½æ•°ï¼ˆéæµå¼ï¼‰
def call_assistant_non_streaming(access_token, message):
    url = "https://chatglm.cn/chatglm/assistant-api/v1/stream_sync"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json"
    }
    payload = {
        "assistant_id": "xxx",
        "prompt": message
    }
    if "conversation_id" in st.session_state:
        payload["conversation_id"] = st.session_state.conversation_id

    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception("Error calling assistant: {}".format(response.text))


def main():
    # ä¾§è¾¹æ æ¨¡å‹é€‰æ‹©
    chosen_model = st.sidebar.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        options=list(MODEL_OPTIONS.keys()),
        index=0  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª
    )

    if chosen_model == "ChatLaw-å¸æ³•æ‘˜è¦":
        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹åç§°è·å–è·¯å¾„
        model_path = MODEL_OPTIONS[chosen_model]
        model, tokenizer = init_model(model_path)
        if model is None or tokenizer is None:
            st.stop()

    # åˆå§‹åŒ–èŠå¤©è®°å½•
    messages = init_chat_history()

    # æ˜¾ç¤ºæ¬¢è¿è¯­ï¼ˆä»…åœ¨é¦–æ¬¡åŠ è½½æ—¶ï¼‰
    if not messages:
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            st.markdown(f"æ‚¨å¥½ï¼Œæˆ‘æ˜¯{chosen_model}ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ’–")

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in messages:
        avatar = "ğŸ™‹â€â™‚ï¸" if message["role"] == "user" else "ğŸ¤–"
        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"])
            # if 'rag' in message:
            #     with st.expander("ğŸ” æŸ¥çœ‹ä¾æ®çš„ RAG å†…å®¹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
            #         for idx, rag in enumerate(message["rag"]):
            #             st.markdown(f"#### æ¥æº {idx + 1}: {rag['title']}")
            #             st.markdown(f"ç›¸å…³æ€§è¯„åˆ†: `{rag['score']:.4f}`")
            #             st.markdown(rag["content"])
            #             st.divider()

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("Shift + Enter æ¢è¡Œï¼ŒEnter å‘é€"):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user", avatar="ğŸ™‹â€â™‚ï¸"):
            st.markdown(prompt)
        messages.append({"role": "user", "content": prompt})

        # ç”Ÿæˆå›å¤
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            try:
                if chosen_model == "ChatLaw-å¸æ³•æ‘˜è¦":
                    # æ„å»ºæ¨¡å‹è¾“å…¥
                    text = tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True,
                    )
                    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

                    # ç”Ÿæˆæ–‡æœ¬
                    generated_ids = model.generate(
                        **model_inputs,
                        max_new_tokens=512,
                        pad_token_id=tokenizer.eos_token_id,
                        temperature=0.95,
                        do_sample=True,
                        top_p=0.7
                    )

                    # è§£ç å¹¶æå–æ–°ç”Ÿæˆå†…å®¹
                    generated_ids = generated_ids[:, model_inputs.input_ids.shape[1]:]
                    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                    # æµå¼æ˜¾ç¤º
                    st.write_stream(response_generator(response))
                    messages.append({"role": "assistant", "content": response})
                else:
                    try:
                        access_token = get_access_token()
                        result = call_assistant_non_streaming(access_token, prompt)
                        # æå–æœ€ç»ˆå›ç­”
                        response = ""
                        final_output = result['result']['output']
                        if "conversation_id" not in st.session_state:
                            st.session_state.conversation_id = result['result']['conversation_id']
                        for item in final_output:
                            for content_item in item['content']:
                                if 'type' in content_item and content_item['type'] == 'text':
                                    response += content_item['text']
                        # æµå¼æ˜¾ç¤º
                        st.write_stream(response_generator(response))
                        # rag_contents = []
                        # for item in final_output:
                        #     for content_item in item['content']:
                        #         if 'type' in content_item and content_item['type'] == 'rag_slices':
                        #             for doc in content_item['content']:
                        #                 rag_contents.append({
                        #                     "title": doc.get("document_name", "æœªçŸ¥æ–‡ä»¶"),
                        #                     "content": doc.get("text", ""),
                        #                     "score": doc.get("score", 0)
                        #                 })

                        # # å±•ç¤º RAG å†…å®¹ï¼ˆå¯æŠ˜å ï¼‰
                        # if rag_contents:
                        #     with st.expander("ğŸ” æŸ¥çœ‹ä¾æ®çš„ RAG å†…å®¹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                        #         for idx, rag in enumerate(rag_contents):
                        #             st.markdown(f"#### æ¥æº {idx + 1}: {rag['title']}")
                        #             st.markdown(f"ç›¸å…³æ€§è¯„åˆ†: `{rag['score']:.4f}`")
                        #             st.markdown(rag["content"])
                        #             st.divider()

                        messages.append({"role": "assistant", "content": response})
                    except Exception as e:
                        print(e)

            except RuntimeError as e:
                if "CUDA out of memory" in str(e):
                    st.error("æ˜¾å­˜ä¸è¶³ï¼Œè¯·ç¼©çŸ­è¾“å…¥æˆ–é‡å¯ä¼šè¯ã€‚")
                else:
                    st.error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")

        # æ¸…ç©ºå¯¹è¯æŒ‰é’®
        st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    main()
